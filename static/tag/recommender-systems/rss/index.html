<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[recommender systems - Rico Meinl]]></title><description><![CDATA[I'm interested in biotech, longevity, machine learning and the decentralized web]]></description><link>https://rmeinl.com/ghost/</link><image><url>https://rmeinl.com/ghost/favicon.png</url><title>recommender systems - Rico Meinl</title><link>https://rmeinl.com/ghost/</link></image><generator>Ghost 4.3</generator><lastBuildDate>Mon, 26 Apr 2021 11:34:50 GMT</lastBuildDate><atom:link href="https://rmeinl.com/ghost/tag/recommender-systems/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Recommender Systems: The Most Valuable Application of Machine Learning (Part 2)]]></title><description><![CDATA[Why Recommender Systems are the most valuable application of Machine Learning and how Machine Learning-driven Recommenders already drive almost every aspect of our lives.]]></description><link>https://rmeinl.com/ghost/recommender-systems-the-most-valuable-application-of-machine-learning-part-2/</link><guid isPermaLink="false">6085751174348916f1b0899e</guid><category><![CDATA[recommender systems]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[data science]]></category><dc:creator><![CDATA[Rico Meinl]]></dc:creator><pubDate>Sun, 04 Oct 2020 20:50:12 GMT</pubDate><content:encoded><![CDATA[<p>Why Recommender Systems are the most valuable application of Machine Learning and how Machine Learning-driven Recommenders already drive almost every aspect of our lives.</p><p><a href="https://towardsdatascience.com/recommender-systems-the-most-valuable-application-of-machine-learning-2bc6903c63ce">Read this article on Medium.</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*F2mBbZRHPXxa3cyg3z8esg.png" class="kg-image" alt loading="lazy"><figcaption>Recommender Systems already drive almost every aspect of our daily&#xA0;lives.</figcaption></figure><hr><p>This is the second part of the article published on 11 May. In the first part I covered:</p><ul><li>Business Value</li><li>Problem Formulation</li><li>Data</li><li>Algorithms</li></ul><p>In this second part I will cover the following topics:</p><ul><li>Evaluation Metrics</li><li>User Interface</li><li>Cold-start Problem</li><li>Exploration vs. Exploitation</li><li>The Future of Recommender Systems</li></ul><p>Throughout this article, I will continue to use examples of the companies that have built the most widely used systems over the last couple of years, including Airbnb, Amazon, Instagram, LinkedIn, Netflix, Spotify, Uber Eats, and YouTube.</p><hr><h3 id="evaluation-metrics">Evaluation Metrics</h3><p>Now that we have the algorithm for our Recommender System, we need to find a way to evaluate its performance. As with every Machine Learning model, there are two types of evaluation:</p><ol><li>Offline Evaluation</li><li>Online Evaluation</li></ol><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*HjqVASOg7qdIUEafmYK9rg.png" class="kg-image" alt loading="lazy"><figcaption>Offline/Online Testing Framework</figcaption></figure><p>Generally speaking, we can consider the Offline Evaluation metrics as <em>low-level</em> metrics, that are usually easily measurable. The most well-known example would be Netflix choosing to use <em>root mean squared error</em> (RMSE) as a proxy metric for their Netflix Prize Challenge. The Online Evaluation metrics are the <em>high-level</em> business metrics that are only measurable as soon as we ship our model into the real world and test it with real users. Some examples include customer retention, click-through rate, or user engagement.</p><h4 id="offline-evaluation">Offline Evaluation</h4><p>As most of the existing Recommender Systems consist of two stages (candidate generation and ranking), we need to pick the right metrics for each stage. For the candidate generation stage,<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" rel="noopener"> YouTube</a>, for instance, focuses on <strong>high precision</strong> so <em>&#x201C;out of all the videos that were pre-selected how many are relevant&#x201D;</em>. This makes sense given that in the first stage we want to filter for a smaller set of videos whilst making sure all of them are potentially relevant to the user. In the second stage, presenting a few &#x201C;best&#x201D; recommendations in a list requires a fine-level representation to distinguish relative importance among candidates with <strong>high recall </strong>(<em>&#x201C;how many of the relevant videos did we find&#x201D;</em>)<strong>.</strong></p><p>Often, most of the examples are using the standard evaluation metrics used in the Machine Learning community: from ranking measures, such as normalized discounted cumulative gain, mean reciprocal rank, or fraction of concordant pairs, to classification metrics including accuracy, precision, recall, or F-score.</p><p><a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener">Instagram formulated</a> the optimization function of their final pass model a little different:</p><blockquote>We predict individual actions that people take on each piece of media, whether they&#x2019;re positive actions such as like and save, or negative actions such as &#x201C;See Fewer Posts Like This&#x201D; (SFPLT). We use a multi-task multi-label (MTML) neural network to predict these events.</blockquote><p>As appealing as offline experiments are, they have a major drawback: they assume that members would have behaved in the same way, for example, playing the same videos, if the new algorithm being evaluated had been used to generate the recommendations. That&#x2019;s why we need online evaluation to measure the actual impact our model has on the higher-level business metrics.</p><h4 id="online-evaluation">Online Evaluation</h4><p>The approach to be aware of here is A/B testing. There are many interesting and exhaustive articles/<a href="https://www.udacity.com/course/ab-testing--ud257" rel="noopener">courses</a> that cover this well, therefore I won&#x2019;t spend too much time on this. The only slight variation I have encountered is Netflix&#x2019;s approach called &#x201C;Consumer Data Science&#x201D; that you can<a href="https://netflixtechblog.com/how-we-determine-product-success-980f81f0047e" rel="noopener"> read about it here</a>.</p><p>The most popular high-level metrics that companies are measuring here are <em>Click-Through Rate</em> and <em>Engagement</em>. Uber Eats goes further here and designed a multi-objective tradeoff that<a href="https://eng.uber.com/uber-eats-recommending-marketplace/" rel="noopener"> captures multiple high-level metrics</a> to account for the overall health of their three-sided marketplace (among others: Marketplace Fairness, Gross Bookings, Reliability, Eater Happiness). In addition to medium-term engagement, Netflix focuses on member retention rates as their online tests can<a href="https://dl.acm.org/doi/pdf/10.1145/2843948" rel="noopener"> range from between 2&#x2013;6 months</a>.</p><p>YouTube famously prioritizes watch-time over click-through rate. They even<a href="https://youtube-creators.googleblog.com/2012/08/youtube-now-why-we-focus-on-watch-time.html" rel="noopener"> wrote an article, explaining why</a>:</p><blockquote>Ranking by click-through rate often promotes deceptive videos that the user does not complete (&#x201C;clickbait&#x201D;) whereas watch time better captures engagement</blockquote><h4 id="evaluating-embeddings">Evaluating Embeddings</h4><p>As covered in the section on algorithms, embeddings are a crucial part of the candidate generation stage. However, unlike with a classification or regression model, it&#x2019;s<a href="https://blog.twitter.com/engineering/en_us/topics/insights/2018/embeddingsattwitter.html" rel="noopener"> notoriously difficult to measure the quality of an embedding</a> given that they are often being used in different contexts. A sanity check we can perform is to map the high-dimensional embedding vector into a lower-dimensional representation (via PCA, t-SNE, or UMAP) or apply clustering techniques such as k-means and then visualize the results. Airbnb did this with their<a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e"> listing embeddings</a> to confirm that listings from similar locations are clustered together.</p><hr><h3 id="user-interface">User Interface</h3><p>For a Machine Learning Engineer or Data Scientist, the probably most overlooked aspect of the equation is the User Interface. The problem is that if your UI does not contain the needed components to showcase the recommendations or showcases them in the wrong context, the feedback loop is inherently flawed.</p><p>Let&#x2019;s take Linkedin as an example to illustrate this. If I&#x2019;m browsing through people&#x2019;s profiles, on the right-hand side of the screen I see recommendations for <em>similar people</em>. When I&#x2019;m browsing through companies, I see recommendations for <em>similar companies</em>. The recommendations are adapted to my current goals and context and encourage me to keep browsing the site. If the <em>similar companies</em> recommendations would appear on a person&#x2019;s profile, I would probably be less encouraged to click on their profile as it is not what I am currently looking for.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*i-3rNsokIOjyoBRgeiOhvA.png" class="kg-image" alt loading="lazy"><figcaption>Similar User Recommendations on&#xA0;Linkedin</figcaption></figure><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*A1NpLcH0HG0RrBB3oOTVKA.png" class="kg-image" alt loading="lazy"><figcaption>Similar Companies Recommendations on&#xA0;Linkedin</figcaption></figure><p>You can build the best Recommender System in the world, however, if your interface is not designed to serve the user&#x2019;s needs and wants, no one will appreciate the recommendations. In fact, the User Interface challenge is so crucial that<a href="https://netflixtechblog.com/learning-a-personalized-homepage-aa8ec670359a" rel="noopener"> Netflix turned all components on their website into dynamic ones</a> which are assembled by a Machine Learning algorithm to best reflect the goals of a user.</p><p>Spotify followed that model and <a href="https://labs.spotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/" rel="noopener">adopted a similar layout for their home screen design</a>, as can be seen below.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*ZopV25d9-x1Gma5-YIbkIA.png" class="kg-image" alt loading="lazy"><figcaption>Personalizing Spotify Home with Machine Learning (Source:&#xA0;<a href="https://www.oreilly.com/radar/personalization-of-spotify-home-and-tensorflow/" data-href="https://www.oreilly.com/radar/personalization-of-spotify-home-and-tensorflow/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Spotify</a>)</figcaption></figure><p>This is an ongoing area where there is still a lot of experimentation. As an example, YouTube recently changed their homepage interface to enable users to narrow down the recommendations for different topics:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*Edcu3SHtcLuF5aZqI1N_rw.png" class="kg-image" alt loading="lazy"><figcaption>New YouTube Home&#xA0;Page</figcaption></figure><hr><h3 id="cold-start-problem">Cold-start Problem</h3><p>The<a href="https://en.wikipedia.org/wiki/Cold_start_%28computing%29" rel="noopener"> cold-start problem</a> is often seen in Recommender Systems because methods such as collaborative filtering rely heavily on past user-item interactions. Companies are confronted with the cold-start problem in two ways: user and item cold-start. Depending on the type of platform, either one of them is more prevalent.</p><h4 id="user-cold-start">User cold-start</h4><p>Imagine a new member signs up for Netflix. At this point, the company doesn&#x2019;t know anything about the new members&#x2019; preferences. How does the company keep her engaged by providing great recommendations?</p><p>In Netflix&#x2019;s case, new members get a one-month free trial, during which cancellation rates are the highest while they decrease quickly after that. This is why any improvements to the cold-start problem present an immense business opportunity for Netflix, in order to increase engagement and retention in those first 30 days. Today, their members are given a survey during the sign-up process, during which they are asked to select videos from an algorithmically populated set that is then used as an input into all of their algorithms.</p><h4 id="item-cold-start">Item cold-start</h4><p>Companies face a similar challenge when new items or content are added to the catalog. Platforms like Netflix or Prime Video hold an existing catalog of media items that changes less frequently (it takes time to create movies or series!), therefore they struggle less with this. On the contrary, on Airbnb or Zillow, new listings are created every day and at that point, they do not have an embedding as they were not present during the training process. Airbnb solves this the following way:</p><blockquote>To create embeddings for a new listing we find 3 geographically closest listings that do have embeddings, and are of same listing type and price range as the new listing, and calculate their mean vector.</blockquote><p>For Zillow, this is especially critical as some of the new home listings might only be on the site for a couple of days. They<a href="https://www.zillow.com/tech/embedding-similar-home-recommendation/" rel="noopener"> creatively solved this problem</a> by creating a neural network-based mapping function from the content space to the embedding space, which is guided by the engagement data from users during the learning phase. This allows them to map a new home listing to the learned embedding space just by using its features.</p><hr><h3 id="exploration-vs-exploitation">Exploration vs. Exploitation</h3><p>The concept of exploration/exploitation can be seen as the balancing of new content with well-established content. I was going to illustrate this concept myself, while I found this great excerpt that hits it right out of the ballpark:</p><blockquote>&#x201C;Imagine you&#x2019;ve just entered an ice cream shop. You now face a crucial decision&#x200A;&#x2014;&#x200A;out of about 30 flavors you need to choose only one!<br>You can go with two strategies: either go with that favorite flavor of yours that you already know is the best; or explore new flavors you never tried before, and maybe find a new best flavor.<br>These two strategies&#x200A;&#x2014;&#x200A;exploitation and exploration&#x200A;&#x2014;&#x200A;can also be used when recommending content. We can either exploit items that have high click-through rate with high certainty&#x200A;&#x2014;&#x200A;maybe because these items have been shown thousands of times to similar users, or we can explore new items we haven&#x2019;t shown to many users in the past. Incorporating exploration into your recommendation strategy is crucial&#x200A;&#x2014;&#x200A;without it, new items don&#x2019;t stand a chance against older, more familiar ones.&#x201D;</blockquote><p><em>(Source: </em><a href="https://anotherdatum.com/exploration-exploitation.html" rel="noopener"><em>Recommender Systems: Exploring the Unknown Using Uncertainty</em></a><em>)</em></p><p>This tradeoff is a typical reinforcement learning problem and a commonly used approach is the multi-armed bandit algorithm. This is used by Spotify for the<a href="http://sigir.org/afirm2019/slides/16.%20Friday%20-%20Music%20Recommendation%20at%20Spotify%20-%20Ben%20Carterette.pdf" rel="noopener"> personalization of each users&#x2019; home page</a> as well as Uber Eats for personalized recommendations<a href="https://eng.uber.com/uber-eats-recommending-marketplace/" rel="noopener"> optimized for their three-sided marketplace</a>. Two scientists at Netflix gave a great talk about how they are<a href="https://www.youtube.com/watch?v=kY-BCNHd_dM" rel="noopener"> using the MAB framework for movie recommendations</a>.</p><p>Though I should mention that this is, by no means, the final solution to this problem, it seems to work for Netflix, Spotify, and Uber Eats, right?</p><p>Yes. But!</p><p>Netflix has roughly 160 million users and about 6.000 movies/shows. Spotify has about 230 million users and 50 million songs + 500.000 podcasts.</p><p>Twitter&#x2019;s 330 million active users generate more than <strong><em>500 million tweets</em></strong> per day (350.000 tweets per minute, 6.000 tweets per second). And then there&#x2019;s YouTube, with its <strong><em>300 hours of videos</em></strong> uploaded every minute!</p><p>The exploration space in the two latter cases is a <em>little</em> bit bigger than in the case of Netflix or Uber Eats, which makes the problem a lot more challenging.</p><hr><h3 id="the-future-of-recommender-systems">The Future of Recommender Systems</h3><p>This is the end of my little survey over Recommender Systems. As we have observed, Recommender Systems already guide so many aspects of our life. All the algorithms we covered over the course of these two articles are competing for our attention every day. And after all, they are all maximizing the time spent on their platform. As I illustrated in the section on Evaluation methods, most of the algorithms are optimizing for something like Click-through rate, engagement, or in YouTube&#x2019;s case: watch time.</p><p><strong><em>What does that mean for us as a consumer?</em></strong></p><p>What it means is, that we are not in control of our desires anymore. While this might sound poetic, think about it. Let&#x2019;s look at YouTube; we all have goals when coming to the site. We might want to listen to music, watch something funny, or learn something new. But all the content that is recommended to us (either through the Home Page recommendations, Search Ranking, or Watch Next) is optimized to keep us on the site for longer.</p><p>Lex Fridman and Fran&#xE7;ois Chollet had a<a href="https://www.youtube.com/watch?v=Bo8MY4JpiXE" rel="noopener"> great conversation about this</a> on the Artificial Intelligence Podcast. Instead of choosing the metric to optimize for, what if companies would put the user in charge of choosing their own objective function? What if they would take the personal goals of the user&#x2019;s profile into account and ask the user, what do you want to achieve? Right now, this technology is almost like our boss and we&#x2019;re not in control of it. Wouldn&#x2019;t it be incredible to leverage the power of Recommender Systems to be more like a mentor, a coach, or an assistant?</p><p>Imagine, as a consumer, you could ask YouTube to optimize the content to maximize learning outcomes. The technology is certainly already there. The challenge would really lie in aligning this with the existing business models and designing the right interface to empower the user to make that choice, and also to change as their goals evolve. With its new interface, YouTube is perhaps already taking baby-steps in that direction by putting the user in charge to select categories that she wants to see recommendations for. But this is just the beginning.</p><p>Could this be the way forward or is this just a consumer&#x2019;s dream?</p><hr><p><strong><em>Resources</em></strong></p><p><a href="https://www.youtube.com/watch?v=Bo8MY4JpiXE" rel="noopener">Fran&#xE7;ois Chollet: Keras, Deep Learning, and the Progress of AI | Artificial Intelligence Podcast</a></p><p><a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e" rel="noopener">Airbnb&#x200A;&#x2014;&#x200A;Listing Embeddings in Search Ranking</a></p><p><a href="https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789" rel="noopener">Airbnb&#x200A;&#x2014;&#x200A;Machine Learning-Powered Search Ranking of Airbnb Experiences</a></p><p><a href="https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf" rel="noopener nofollow noopener noopener">Amazon&#x200A;&#x2014;&#x200A;Amazon.com Recommendations Item-to-Item Collaborative Filtering</a></p><p><a href="https://www.amazon.science/the-history-of-amazons-recommendation-algorithm" rel="noopener nofollow noopener noopener">Amazon&#x200A;&#x2014;&#x200A;The history of Amazon&#x2019;s recommendation algorithm</a></p><p><a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener nofollow noopener noopener">Instagram&#x200A;&#x2014;&#x200A;Powered by AI: Instagram&#x2019;s Explore recommender system</a></p><p><a href="https://ls13-www.cs.tu-dortmund.de/homepage/rsweb2014/papers/rsweb2014_submission_3.pdf" rel="noopener nofollow noopener noopener">LinkedIn&#x200A;&#x2014;&#x200A;The Browsemaps: Collaborative Filtering at LinkedIn</a></p><p><a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429" rel="noopener nofollow noopener noopener">Netflix&#x200A;&#x2014;&#x200A;Netflix Recommendations: Beyond the 5 stars (Part 1)</a></p><p><a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5" rel="noopener nofollow noopener noopener">Netflix&#x200A;&#x2014;&#x200A;Netflix Recommendations: Beyond the 5 stars (Part 2)</a></p><p><a href="https://dl.acm.org/doi/pdf/10.1145/2843948" rel="noopener nofollow noopener noopener">Netflix&#x200A;&#x2014;&#x200A;The Netflix Recommender System: Algorithms, Business Value, and Innovation</a></p><p><a href="https://netflixtechblog.com/learning-a-personalized-homepage-aa8ec670359a" rel="noopener nofollow noopener noopener">Netflix&#x200A;&#x2014;&#x200A;Learning a Personalized Homepage</a></p><p><a href="https://pdfs.semanticscholar.org/f635/6c70452b3f56dc1ae07b4649a80239afb1b6.pdf" rel="noopener nofollow noopener noopener">Pandora&#x200A;&#x2014;&#x200A;Pandora&#x2019;s Music Recommender</a></p><p><a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe" rel="noopener">Spotify&#x200A;&#x2014;&#x200A;Discover Weekly: How Does Spotify Know You So Well?</a></p><p><a href="https://labs.spotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/" rel="noopener nofollow noopener noopener">Spotify&#x200A;&#x2014;&#x200A;For Your Ears Only: Personalizing Spotify Home with Machine Learning</a></p><p><a href="https://www.slideshare.net/MrChrisJohnson/from-idea-to-execution-spotifys-discover-weekly/31-1_0_0_0_1" rel="noopener nofollow noopener noopener">Spotify&#x200A;&#x2014;&#x200A;From Idea to Execution: Spotify&#x2019;s Discover Weekly</a></p><p><a href="https://blog.twitter.com/engineering/en_us/topics/insights/2018/embeddingsattwitter.html" rel="noopener nofollow noopener noopener">Twitter&#x200A;&#x2014;&#x200A;Embeddings@Twitter</a></p><p><a href="https://eng.uber.com/uber-eats-recommending-marketplace/" rel="noopener nofollow noopener noopener">Uber Eats&#x200A;&#x2014;&#x200A;Food Discovery with Uber Eats: Recommending for the Marketplace</a></p><p><a href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener nofollow noopener noopener">Uber Eats&#x200A;&#x2014;&#x200A;Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations</a></p><p><a href="https://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf" rel="noopener nofollow noopener noopener">YouTube&#x200A;&#x2014;&#x200A;The YouTube Video Recommendation System</a></p><p><a href="https://arxiv.org/pdf/1409.2944.pdf" rel="noopener nofollow noopener noopener">YouTube&#x200A;&#x2014;&#x200A;Collaborative Deep Learning for Recommender Systems</a></p><p><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" rel="noopener nofollow noopener noopener">YouTube&#x200A;&#x2014;&#x200A;Deep Neural Networks for YouTube Recommendations</a></p><p><a href="https://www.zillow.com/tech/embedding-similar-home-recommendation/" rel="noopener nofollow noopener noopener">Zillow&#x200A;&#x2014;&#x200A;Home Embeddings for Similar Home Recommendations</a></p><p><a href="https://www.youtube.com/watch?v=giIXNoiqO_U&amp;list=PL-6SiIrhTAi6x4Oq28s7yy94ubLzVXabj" rel="noopener nofollow noopener noopener">Andrew Ng&#x2019;s Machine Learning Course (Recommender Systems)</a></p><p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture" rel="noopener nofollow noopener noopener">Google&#x2019;s Machine Learning Crash Course&#x200A;&#x2014;&#x200A;Embeddings</a></p><hr>]]></content:encoded></item><item><title><![CDATA[Recommender Systems: The Most Valuable Application of Machine Learning (Part 1)]]></title><description><![CDATA[Why Recommender Systems are the most valuable application of Machine Learning and how Machine Learning-driven Recommenders already drive almost every aspect of our lives.]]></description><link>https://rmeinl.com/ghost/recommender-systems-the-most-valuable-application-of-machine-learning-part-1/</link><guid isPermaLink="false">6085751174348916f1b0899d</guid><category><![CDATA[recommender systems]]></category><category><![CDATA[machine learning]]></category><category><![CDATA[data science]]></category><dc:creator><![CDATA[Rico Meinl]]></dc:creator><pubDate>Sun, 04 Oct 2020 20:46:09 GMT</pubDate><content:encoded><![CDATA[<p>Why Recommender Systems are the most valuable application of Machine Learning and how Machine Learning-driven Recommenders already drive almost every aspect of our lives.</p><p><a href="https://towardsdatascience.com/recommender-systems-the-most-valuable-application-of-machine-learning-part-1-f96ecbc4b7f5">Read this article on Medium.</a></p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*F2mBbZRHPXxa3cyg3z8esg.png" class="kg-image" alt loading="lazy"><figcaption>Recommender Systems already drive almost every aspect of our daily&#xA0;lives.</figcaption></figure><hr><p>Look back at your week: a Machine Learning algorithm determined what songs you might like to listen to, what food to order online, what posts you see on your favorite social networks, as well as the next person you may want to connect with, what series or movies you would like to watch, etc&#x2026;</p><p>Machine Learning already guides so many aspects of our life without us necessarily being conscious of it. All of the applications mentioned above are driven by one type of algorithm: recommender systems.</p><p>In this article, I will explore and dive deeper into all the aspects that come into play to build a successful recommender system. The length of this article got a little out of hand so I decided to split it into two parts. This first part will cover:</p><ul><li>Business Value</li><li>Problem Formulation</li><li>Data</li><li>Algorithms</li></ul><p>The Second Part will cover:</p><ul><li>Evaluation Metrics</li><li>User Interface</li><li>Cold-start Problem</li><li>Exploration vs. Exploitation</li><li>The Future of Recommender Systems</li></ul><p>Throughout this article, I will be using examples of the companies that have built the most widely used systems over the last couple of years, including Airbnb, Amazon, Instagram, LinkedIn, Netflix, Spotify, Uber Eats, and YouTube.</p><hr><h3 id="business-value">Business Value</h3><p>Harvard Business Review made a strong statement by calling Recommenders the <a href="https://hbr.org/2017/08/great-digital-companies-build-great-recommendation-engines" rel="noopener">single most important algorithmic distinction between &#x201C;born digital&#x201D; enterprises and legacy companies</a>. HBR also described the virtuous business cycle these can generate: the more people use a company&#x2019;s Recommender System, the more valuable they become and the more valuable they become, the more people use them.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*U_eUe0NBy7uQPA10SFY-VQ.png" class="kg-image" alt loading="lazy"><figcaption>The Virtuous Business Cycle of Recommender Systems (source: <a href="https://www.mdpi.com/2199-8531/5/3/44/htm" data-href="https://www.mdpi.com/2199-8531/5/3/44/htm" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">MDPI</a>,&#xA0;CC)</figcaption></figure><p>We are encouraged to look at recommender systems, not as a way to sell more online, but rather to see it as a renewable resource for <em>relentlessly improving customer insights and our own insights as well</em>. If we look at the illustration above, we can see that many legacy companies also have tons of users and therefore tons of data. The reason their virtuous cycle has not picked up as much as the ones off Amazon, Netflix or Spotify is because of the lack of knowledge on how to convert their user data into actionable insights, which can then be used to improve their product or services.</p><p>Looking at Netflix, for example, shows how crucial this is, as 80% of what people watch comes from some sort of recommendation. In <a href="https://dl.acm.org/doi/pdf/10.1145/2843948" rel="noopener">2015, one of their papers quoted</a>:</p><blockquote>&#x201C;We think the combined effect of personalization and recommendations save us more than $1B per year.&#x201D;</blockquote><p>If we look at Amazon, <a href="https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers" rel="noopener">35% of what customers purchase at Amazon</a> comes from product recommendations and at Airbnb, Search Ranking and Similar Listings drive <a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e">99% of all booking conversions</a>.</p><hr><h3 id="problem-formulation">Problem Formulation</h3><p>Now that we&#x2019;ve seen the immense value, companies can gain from Recommender Systems, let&#x2019;s look at the type of challenges that can be solved by them. Generally speaking, tech companies are trying to recommend the <strong>most relevant content</strong> to their users. That could mean:</p><ul><li>similar home listings (Airbnb, Zillow)</li><li>relevant media, e.g. photos, videos and stories (Instagram)</li><li>relevant series and movies (Netflix, Amazon Prime Video)</li><li>relevant songs and podcasts (Spotify)</li><li>relevant videos (YouTube)</li><li>similar users, posts (LinkedIn, Twitter, Instagram)</li><li>relevant dishes and restaurants (Uber Eats)</li></ul><p>The formulation of the problem is critical here. Most of the time, companies want to recommend content that users are most likely to enjoy in the future. The reformulation of this problem, as well as the algorithmic changes from recommending &#x201C;what users are most likely to watch&#x201D; to &#x201C;what users are most likely to watch <em>in the future</em>&#x201D; <a href="https://www.amazon.science/the-history-of-amazons-recommendation-algorithm" rel="noopener">allowed Amazon PrimeVideo to gain a 2x improvement</a>, a &#x201C;once-in-a-decade leap&#x201D; for their movie Recommender System.</p><blockquote>&#x201C;Amazon researchers found that using neural networks to generate movie recommendations worked much better when they sorted the input data chronologically and used it to predict future movie preferences over a short (one- to two-week) period.&#x201D;</blockquote><hr><h3 id="data">Data</h3><p>Recommender Systems usually take two types of data as input:</p><ul><li><strong>User Interaction Data </strong>(Implicit/Explicit)</li><li><strong>Item Data</strong> (Features)</li></ul><p>The &#x201C;classic&#x201D;, and still widely used approach to recommender systems based on <strong>collaborative filtering</strong> (used by <a href="https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf" rel="noopener">Amazon</a>, <a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429" rel="noopener">Netflix</a>, <a href="https://ls13-www.cs.tu-dortmund.de/homepage/rsweb2014/papers/rsweb2014_submission_3.pdf" rel="noopener">LinkedIn</a>, <a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe">Spotify</a> and <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" rel="noopener">YouTube</a>) uses either User-User or Item-Item relationships to find similar content. I&#x2019;m not going to go deeper into the inner workings of this, as there are a lot of articles on that topic&#x200A;&#x2014;&#x200A;<a href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-recommendation-engine-python/" rel="noopener">like this one</a>&#x200A;&#x2014;&#x200A;that explain this concept well.</p><p>The <em>user interaction data</em> is the data we gather from the weblogs and can be divided into two groups:</p><p><strong><em>Explicit data</em></strong>: explicit input from our users (e.g. movie ratings, search logs, liked, commented, watched, favorited, etc.)</p><p><strong><em>Implicit data</em></strong>: information that is not provided intentionally but gathered from available data streams (e.g. search history, order history, clicked on, accounts interacted with, etc.)</p><p>The <em>item data</em> consists mainly of an item&#x2019;s features. In YouTube&#x2019;s case that would be a video&#x2019;s metadata such as title and description. For Zillow, this could be a home&#x2019;s Zip Code, City Region, Price, or Number of Bedrooms for instance.</p><p>Other data sources could be <strong><em>external data</em></strong> (for example, Netflix might <a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5" rel="noopener">add external item data features</a> such as box office performance or critic reviews) or <strong>expert-generated data</strong> (Pandora&#x2019;s <a href="https://pdfs.semanticscholar.org/f635/6c70452b3f56dc1ae07b4649a80239afb1b6.pdf" rel="noopener">Music Genome Project</a> uses human input to apply values for each song in each of approximately 400 musical attributes).</p><p>A key insight here is that obviously, having more data about your users will inevitably lead to better model results (if applied correctly), however, as Airbnb shows in their <a href="https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789">3-part journey to building a Ranking Model for Airbnb Experiences</a> you can already achieve quite a lot with lesser data: the team at Airbnb already improved bookings by +13% with just 500 experiences and 50k training data size.</p><blockquote>&#x201C;The main take-away is: <em>Don&#x2019;t wait until you have big data, you can do quite a bit with small data to help grow and improve your business.</em>&#x201D;</blockquote><hr><h3 id="algorithms">Algorithms</h3><p>Often, we associate Recommender Systems with just collaborative filtering. That&#x2019;s fair, as in the past this has been the go-to method for a lot of the companies that have deployed successful systems in practice. Amazon was probably the first company to leverage <a href="https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf" rel="noopener">item-to-item collaborative filtering</a>. When they first released the inner workings of their method in a paper in 2003, the system had already been in use for six years.</p><p>Then, in 2006 Netflix followed suit with its famous Netflix Price Challenge which offered $1 million to whoever improved the accuracy of their existing system called <em>Cinematch</em> by 10%. Collaborative filtering was also a part of the early Recommender Systems at <a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe">Spotify</a> and <a href="https://arxiv.org/pdf/1409.2944.pdf" rel="noopener">YouTube</a>. LinkedIn even developed a horizontal collaborative filtering infrastructure, <a href="https://ls13-www.cs.tu-dortmund.de/homepage/rsweb2014/papers/rsweb2014_submission_3.pdf" rel="noopener">known as Browsemaps</a>. This platform enables rapid development, deployment, and computation of collaborative filtering recommendations for almost any use case on LinkedIn.</p><p>If you want to know more about collaborative filtering, I would recommend checking out <a href="https://www.youtube.com/watch?v=giIXNoiqO_U&amp;list=PL-6SiIrhTAi6x4Oq28s7yy94ubLzVXabj" rel="noopener">Section 16 of Andrew Ng&#x2019;s Machine Learning course on Coursera</a> where he goes deeper into the math behind it.</p><p>Now, I would like to take a step back and generalize the concept of a Recommender System. While many companies used to rely on collaborative filtering, today there are a lot of other different algorithms at play that either complement or even replaced the collaborative filtering approach. Netflix went through this change when they shifted from a DVD shipping to a streaming business. As described in one of their papers:</p><blockquote>&#x201C;We indeed relied on such an algorithm heavily when our main business was shipping DVDs by mail, partly because in that context, a star rating was the main feedback that we received that a member had actually watched the video. [&#x2026;] But the days when stars and DVDs were the focus of recommendations at Netflix have long passed. [&#x2026;] Now, our recommender system consists of a variety of algorithms that collectively define the Netflix experience, most of which come together on the Netflix homepage.&#x201D;</blockquote><p>If we zoom out a little bit and look at Recommender Systems more broadly we find that they essentially consist of two parts:</p><ol><li><strong>Candidate Generation</strong></li><li><strong>Ranking</strong></li></ol><p>I am going to use <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" rel="noopener">YouTube&#x2019;s Recommender System</a> as an example below as they provided a good visualization, but that very same concept is applied by Instagram for <a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener">recommendations in &#x201C;Instagram Explore&#x201D;</a>, by Uber Eats in their <a href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener">Dish and Restaurant Recommender System</a>, by Netflix for their <a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5" rel="noopener">movie recommendations </a>and probably many other companies.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://cdn-images-1.medium.com/max/1600/1*6LG9QN2XEtK6UCOZG4cavA.png" class="kg-image" alt loading="lazy"><figcaption>2-stage Recommender System (inspired by&#xA0;<a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" data-href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">YouTube</a>)</figcaption></figure><p>According to Netflix, the goal of Recommender Systems is to present a number of attractive items for a person to choose from. This is usually accomplished by selecting some items (<em>candidate generation</em>) and sorting them (<em>ranking</em>) in the order of expected enjoyment (or utility).</p><p>Let&#x2019;s further investigate the two stages:</p><h4 id="candidate-generation">Candidate Generation</h4><p>In this stage, we want to source the relevant candidates that could be eligible to show to our users. Here, we are working with the whole catalog of items so it can be quite large (YouTube and Instagram are great examples here). The key to doing this is entity embeddings. What are entity embeddings?</p><p>An entity embedding is a mathematical vector representation of an entity such that its dimensions might represent certain properties. Twitter has a great example of this in a <a href="https://blog.twitter.com/engineering/en_us/topics/insights/2018/embeddingsattwitter.html" rel="noopener">blog post about Embeddings@Twitter</a>: say we have two NBA players (Stephen Curry and LeBron James) and two musicians (Kendrick Lamar and Bruno Mars). We expect the distance between the embeddings of the NBA players to be smaller than the distance between the embeddings of a player and a musician. We can calculate the distance between two embeddings using the formula for Euclidean distance.</p><p><strong><em>How do we come up with these embeddings?</em></strong></p><p>Well, one way to do this would be collaborative filtering. We have our items and our users. If we put them in a matrix (for the example of Spotify) it could look like this:</p><figure class="kg-card kg-image-card"><img src="https://cdn-images-1.medium.com/max/1600/1*dkvXGVpAlK25F-WznatMMA.png" class="kg-image" alt loading="lazy"></figure><p>After applying the <a href="https://www.slideshare.net/MrChrisJohnson/from-idea-to-execution-spotifys-discover-weekly/31-1_0_0_0_1" rel="noopener">matrix factorization algorithm</a>, we end up with user vectors and song vectors. To find out which users&#x2019; tastes are most similar to another&#x2019;s, collaborative filtering compares one users&#x2019; vector with all of the other users&#x2019; vectors, ultimately spitting out which users are the closest matches. The same goes for the Y vector, <em>songs</em>: you can compare a single song&#x2019;s vector with all the others, and find out which songs are most similar to the one in question.</p><p>Another way to do this takes inspiration from applications in the domain of Natural Language Processing. Researchers generalized the <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener">word2vec algorithm</a>, developed by Google in the early 2010s to all entities appearing in a similar context. In word2vec, the networks are trained by directly taking into account the word order and their co-occurrence, based on the assumption that words frequently appearing together in the sentences also share more statistical dependence. As Airbnb describes, in their <a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e">blog post about creating Listing Embeddings</a>:</p><blockquote>More recently, the concept of embeddings has been extended beyond word representations to other applications outside of NLP domain. Researchers from the Web Search, E-commerce and Marketplace domains have realized that just like one can train word embeddings by treating a sequence of words in a sentence as context, the same can be done for training embeddings of user actions by treating sequence of user actions as context. Examples include learning representations of <a href="https://arxiv.org/pdf/1606.07154.pdf" rel="noopener nofollow noopener noopener noopener">items that were clicked or purchased</a> or <a href="https://arxiv.org/pdf/1607.01869.pdf" rel="noopener nofollow noopener noopener noopener">queries and ads that were clicked</a>. These embeddings have subsequently been leveraged for a variety of recommendations on the Web.</blockquote><p>Apart from Airbnb, this concept is used by Instagram (IG2Vec) to<a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener"> learn account embeddings</a>, by YouTube to <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" rel="noopener">learn video embeddings</a> and by Zillow to <a href="https://www.zillow.com/tech/embedding-similar-home-recommendation/" rel="noopener">learn categorical embeddings</a>.</p><p>Another, more novel approach to this is called Graph Learning and it is <a href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener">used by Uber Eats for their dish and restaurant embeddings</a>. They represent each of their dishes and restaurant in a separate graph and apply the <a href="http://snap.stanford.edu/graphsage/" rel="noopener">GraphSAGE algorithm</a> to obtain the representations (embeddings) of the respective nodes.</p><p>And last but not least, we can also learn an embedding as part of the neural network for our target task. This approach gets you an embedding well customized for your particular system, but may take longer than training the embedding separately. The <a href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener">Keras Embedding Layer</a> would be one way to achieve this. Google covers this well as part of their <a href="https://developers.google.com/machine-learning/crash-course/embeddings/obtaining-embeddings" rel="noopener">Machine Learning Crash Course.</a></p><p>Once we have this vectorial representation of our items we can simply use Nearest Neighbour Search to find our potential candidates. <br><a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener">Instagram, for example</a>, defines a couple of seed accounts (accounts that people have interacted with in the past) and uses their IG2Vec account embeddings to find similar accounts that are like those. Based on these accounts, they are able to find the media that these accounts posted or engaged with. By doing that, they are able to filter billions of media items down to a couple thousand and then sample 500 candidates from the pool and send those candidates downstream to the ranking stage.</p><p>This phase can also be guided by business rules or just user input (the more information we have the more specific we can be). As Uber Eats mentions <a href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener">in one of their blog posts</a>, for instance, pre-filtering can be based on factors such as geographical location.</p><p>So, to summarize:</p><p><em>In the candidate generation (or sourcing) phase, we filter our whole content catalog for a smaller subset of items that our users might be interested in. To do this we need to map our items into a mathematical representation called embeddings so we can use a similarity function to find the most similar items in space. There are several ways to achieve this. Three of them being collaborative filtering, word2vec for entities, and graph learning.</em></p><h4 id="ranking">Ranking</h4><p>Let&#x2019;s loop back to the case of Instagram. After the candidate generation stage, we have about 500 media items that are potentially relevant and that we could show to a user in their &#x201C;Explore&#x201D; feed. <br>But which ones are going to be the <strong>most relevant</strong>?</p><p>Because, after all, there are only 25 spots on the first page of the &#x201C;Explore&#x201D; section. And if the first items suck, the user is not going to be impressed nor intrigued to keep browsing. Netflix&#x2019;s and Amazon PrimeVideo&#x2019;s web interface <a href="https://www.amazon.science/the-history-of-amazons-recommendation-algorithm" rel="noopener">shows only the top 6 recommendations on the first page</a> associated with each title in its catalog. <a href="https://www.slideshare.net/MrChrisJohnson/from-idea-to-execution-spotifys-discover-weekly/31-1_0_0_0_1" rel="noopener">Spotify&#x2019;s Discover Weekly</a> Playlist contains only 30 songs. <br>Also, all of this is subject to the users&#x2019; device. Smartphones, of course, allowing for less space for relevant recommendations than a web browser.</p><p>&#x201C;There are many ways one could construct a ranking function ranging from simple scoring methods, to pairwise preferences, to optimization over the entire ranking. If we were to formulate this as a Machine Learning problem, we could select positive and negative examples from our historical data and let a Machine Learning algorithm learn the weights that optimize our goal. This family of Machine Learning problems is known as &#x201C;<a href="http://en.wikipedia.org/wiki/Learning_to_rank" rel="noopener">Learning to rank</a>&#x201D; and is central to application scenarios such as search engines or ad targeting. In the ranking stage, we are not aiming for our items to have a global notion of <em>relevance</em>, but rather look for ways of optimizing a personalized model&#x201D; <em>(Extract from </em><a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5" rel="noopener"><em>Netflix Blog Post</em></a><em>).</em></p><p>To accomplish this, <a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener">Instagram uses a three-stage ranking infrastructure</a> to help balance the trade-offs between ranking relevance and computation efficiency. In the <a href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener">case of Uber Eats</a>, their personalized ranking system is &#x201C;a fully-fledged ML model that ranks the pre-filtered dish and restaurant candidates based on additional contextual information, such as the day, time, and current location of the user when they open the Uber Eats app&#x201D;. In general, the level of complexity for your model really depends on the size of your feature space. Many supervised classification methods can be used for ranking. Typical choices include Logistic Regression, Support Vector Machines, Neural Networks, or Decision Tree-based methods such as Gradient Boosted Decision Trees (GBDT). On the other hand, a great number of algorithms specifically designed for learning to rank have appeared in recent years such as RankSVM or RankBoost.</p><p>To summarise:</p><p><em>After selecting initial candidates for our recommendations, in the ranking stage, we need to design a ranking function that ranks items by their relevance. This can be formulated as a Machine Learning problem, and the goal here is to optimize a personalized model for each user. This step is important because in most interfaces we have limited space to recommend items so we need to make the best use of that space by putting the most relevant items at the very top.</em></p><h4 id="baseline">Baseline</h4><p>As for every Machine Learning algorithm, we need a good baseline to measure the improvement of any change. A good baseline to start with is just to use the <a href="https://www.amazon.science/the-history-of-amazons-recommendation-algorithm" rel="noopener">most popular items in the catalog, as described by Amazon</a>:</p><blockquote>&#x201C;In the recommendations world, there&#x2019;s a cardinal rule. If I know nothing about you, then the best things to recommend to you are the most popular things in the world.&#x201D;</blockquote><p>However, if you don&#x2019;t even know what is most popular, because you just launched a new product or new items&#x200A;&#x2014;&#x200A;as was the case with <a href="https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789">Airbnb Experiences</a>&#x200A;&#x2014;&#x200A;you can just randomly re-rank the item collection daily until you have gathered enough data for your first model.</p><hr><p>That&#x2019;s a wrap for Part 1 of this series. There are a couple of points I wanted to emphasize in this article:</p><ul><li>Recommender Systems are the most valuable application of Machine Learning as they are able to create a Virtuous Feedback Loop: the more people use a company&#x2019;s Recommender System, the more valuable they become and the more valuable they become, the more people use them. Once you enter that Loop, the Sky is the Limit.</li><li>The right Problem Formulation is key.</li><li>In the Netflix Price Challenge, teams tried to build models that predict a users&#x2019; rating for a given movie. In the &#x201C;real world&#x201D;, companies use much more sophisticated data inputs which can be classified into two categories: Explicit and Implicit Data.</li><li>In today&#x2019;s world, Recommender Systems rely on much more than just Collaborative Filtering.</li></ul><p>In the Second Part I will cover:</p><ul><li>Evaluation Metrics</li><li>User Interface</li><li>Cold-start Problem</li><li>Exploration vs. Exploitation</li></ul><hr><p><strong><em>Resources</em></strong></p><p><a href="https://medium.com/airbnb-engineering/listing-embeddings-for-similar-listing-recommendations-and-real-time-personalization-in-search-601172f7603e">Airbnb&#x200A;&#x2014;&#x200A;Listing Embeddings in Search Ranking</a></p><p><a href="https://medium.com/airbnb-engineering/machine-learning-powered-search-ranking-of-airbnb-experiences-110b4b1a0789">Airbnb&#x200A;&#x2014;&#x200A;Machine Learning-Powered Search Ranking of Airbnb Experiences</a></p><p><a href="https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf" rel="noopener">Amazon&#x200A;&#x2014;&#x200A;Amazon.com Recommendations Item-to-Item Collaborative Filtering</a></p><p><a href="https://www.amazon.science/the-history-of-amazons-recommendation-algorithm" rel="noopener">Amazon&#x200A;&#x2014;&#x200A;The history of Amazon&#x2019;s recommendation algorithm</a></p><p><a href="https://ai.facebook.com/blog/powered-by-ai-instagrams-explore-recommender-system/" rel="noopener">Instagram&#x200A;&#x2014;&#x200A;Powered by AI: Instagram&#x2019;s Explore recommender system</a></p><p><a href="https://ls13-www.cs.tu-dortmund.de/homepage/rsweb2014/papers/rsweb2014_submission_3.pdf" rel="noopener">LinkedIn&#x200A;&#x2014;&#x200A;The Browsemaps: Collaborative Filtering at LinkedIn</a></p><p><a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429" rel="noopener">Netflix&#x200A;&#x2014;&#x200A;Netflix Recommendations: Beyond the 5 stars (Part 1)</a></p><p><a href="https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-2-d9b96aa399f5" rel="noopener">Netflix&#x200A;&#x2014;&#x200A;Netflix Recommendations: Beyond the 5 stars (Part 2)</a></p><p><a href="https://dl.acm.org/doi/pdf/10.1145/2843948" rel="noopener">Netflix&#x200A;&#x2014;&#x200A;The Netflix Recommender System: Algorithms, Business Value, and Innovation</a></p><p><a href="https://netflixtechblog.com/learning-a-personalized-homepage-aa8ec670359a" rel="noopener">Netflix&#x200A;&#x2014;&#x200A;Learning a Personalized Homepage</a></p><p><a href="https://pdfs.semanticscholar.org/f635/6c70452b3f56dc1ae07b4649a80239afb1b6.pdf" rel="noopener">Pandora&#x200A;&#x2014;&#x200A;Pandora&#x2019;s Music Recommender</a></p><p><a href="https://medium.com/s/story/spotifys-discover-weekly-how-machine-learning-finds-your-new-music-19a41ab76efe">Spotify&#x200A;&#x2014;&#x200A;Discover Weekly: How Does Spotify Know You So Well?</a></p><p><a href="https://labs.spotify.com/2020/01/16/for-your-ears-only-personalizing-spotify-home-with-machine-learning/" rel="noopener">Spotify&#x200A;&#x2014;&#x200A;For Your Ears Only: Personalizing Spotify Home with Machine Learning</a></p><p><a href="https://www.slideshare.net/MrChrisJohnson/from-idea-to-execution-spotifys-discover-weekly/31-1_0_0_0_1" rel="noopener">Spotify&#x200A;&#x2014;&#x200A;From Idea to Execution: Spotify&#x2019;s Discover Weekly</a></p><p><a href="https://blog.twitter.com/engineering/en_us/topics/insights/2018/embeddingsattwitter.html" rel="noopener">Twitter&#x200A;&#x2014;&#x200A;Embeddings@Twitter</a></p><p><a href="https://eng.uber.com/uber-eats-recommending-marketplace/" rel="noopener">Uber Eats&#x200A;&#x2014;&#x200A;Food Discovery with Uber Eats: Recommending for the Marketplace</a></p><p><a href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener">Uber Eats&#x200A;&#x2014;&#x200A;Food Discovery with Uber Eats: Using Graph Learning to Power Recommendations</a></p><p><a href="https://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf" rel="noopener">YouTube&#x200A;&#x2014;&#x200A;The YouTube Video Recommendation System</a></p><p><a href="https://arxiv.org/pdf/1409.2944.pdf" rel="noopener">YouTube&#x200A;&#x2014;&#x200A;Collaborative Deep Learning for Recommender Systems</a></p><p><a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf" rel="noopener">YouTube&#x200A;&#x2014;&#x200A;Deep Neural Networks for YouTube Recommendations</a></p><p><a href="https://www.zillow.com/tech/embedding-similar-home-recommendation/" rel="noopener">Zillow&#x200A;&#x2014;&#x200A;Home Embeddings for Similar Home Recommendations</a></p><p><a href="https://www.youtube.com/watch?v=giIXNoiqO_U&amp;list=PL-6SiIrhTAi6x4Oq28s7yy94ubLzVXabj" rel="noopener">Andrew Ng&#x2019;s Machine Learning Course (Recommender Systems)</a></p><p><a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture" rel="noopener">Google&#x2019;s Machine Learning Crash Course&#x200A;&#x2014;&#x200A;Embeddings</a></p>]]></content:encoded></item></channel></rss>